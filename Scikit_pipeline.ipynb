{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import numpy as np\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import csv\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "#########################################################################################################################\n",
    "\n",
    "# This block contains all of the functions and classes\n",
    "\n",
    "#########################################################################################################################\n",
    "#########################################################################################################################\n",
    "\n",
    "# Function to create Corpus \n",
    "\n",
    "def Corpus(filelist,root):\n",
    "    count = len(filelist)\n",
    "    corpus = [None] * count\n",
    "    for i in range(count):\n",
    "        fid = open(root + filelist[i], 'r' ,encoding='utf-8')\n",
    "        text = fid.read()\n",
    "        corpus[i] = text.replace('\\n','')\n",
    "        fid.close()\n",
    "    return corpus\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "# Delete stop words\n",
    "\n",
    "def delStopWords(corpus):\n",
    "\n",
    "    # List of stop-words from SEO PowerSuite\n",
    "    \n",
    "    stopwords_list = [\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"ain\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren\",\n",
    "                      \"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\n",
    "                      \"can\",\"couldn\",\"couldn't\",\"d\",\"did\",\"didn\",\"didn't\",\"do\",\"does\",\"doesn\",\"doesn't\",\"doing\",\"don\",\n",
    "                      \"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn\",\"hadn't\",\"has\",\"hasn\",\n",
    "                      \"hasn't\",\"have\",\"haven\",\"haven't\",\"having\",\"he\",\"her\",\"here\",\"hers\",\"herself\",\"him\",\"himself\",\n",
    "                      \"his\",\"how\",\"i\",\"if\",\"in\",\"into\",\"is\",\"isn\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"just\",\"ll\",\n",
    "                      \"m\",\"ma\",\"me\",\"mightn\",\"mightn't\",\"more\",\"most\",\"mustn\",\"mustn't\",\"my\",\"myself\",\"needn\",\"needn't\",\n",
    "                      \"no\",\"nor\",\"not\",\"now\",\"o\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\n",
    "                      \"out\",\"over\",\"own\",\"re\",\"s\",\"same\",\"shan\",\"shan't\",\"she\",\"she's\",\"should\",\"should've\",\"shouldn\",\n",
    "                      \"shouldn't\",\"so\",\"some\",\"such\",\"t\",\"than\",\"that\",\"that'll\",\"the\",\"their\",\"theirs\",\"them\",\n",
    "                      \"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\n",
    "                      \"up\",\"ve\",\"very\",\"was\",\"wasn\",\"wasn't\",\"we\",\"were\",\"weren\",\"weren't\",\"what\",\"when\",\"where\",\n",
    "                      \"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\"y\",\"you\",\n",
    "                      \"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"could\",\"he'd\",\"he'll\",\n",
    "                      \"he's\",\"here's\",\"how's\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"let's\",\"ought\",\"she'd\",\"she'll\",\"that's\",\n",
    "                      \"there's\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"what's\",\"when's\",\n",
    "                      \"where's\",\"who's\",\"why's\",\"would\",\"able\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\n",
    "                      \"act\",\"actually\",\"added\",\"adj\",\"affected\",\"affecting\",\"affects\",\"afterwards\",\"ah\",\"almost\",\"alone\",\n",
    "                      \"along\",\"already\",\"also\",\"although\",\"always\",\"among\",\"amongst\",\"announce\",\"another\",\"anybody\",\n",
    "                      \"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apparently\",\"approximately\",\n",
    "                      \"arent\",\"arise\",\"around\",\"aside\",\"ask\",\"asking\",\"auth\",\"available\",\"away\",\"awfully\",\"b\",\"back\",\n",
    "                      \"became\",\"become\",\"becomes\",\"becoming\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\"begins\",\n",
    "                      \"behind\",\"believe\",\"beside\",\"besides\",\"beyond\",\"biol\",\"brief\",\"briefly\",\"c\",\"ca\",\"came\",\"cannot\",\n",
    "                      \"can't\",\"cause\",\"causes\",\"certain\",\"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\n",
    "                      \"contains\",\"couldnt\",\"date\",\"different\",\"done\",\"downwards\",\"due\",\"e\",\"ed\",\"edu\",\"effect\",\"eg\",\n",
    "                      \"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\"enough\",\"especially\",\"et\",\"etc\",\n",
    "                      \"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"except\",\"f\",\"far\",\n",
    "                      \"ff\",\"fifth\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"former\",\"formerly\",\"forth\",\n",
    "                      \"found\",\"four\",\"furthermore\",\"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\n",
    "                      \"go\",\"goes\",\"gone\",\"got\",\"gotten\",\"h\",\"happens\",\"hardly\",\"hed\",\"hence\",\"hereafter\",\"hereby\",\n",
    "                      \"herein\",\"heres\",\"hereupon\",\"hes\",\"hi\",\"hid\",\"hither\",\"home\",\"howbeit\",\"however\",\"hundred\",\"id\",\n",
    "                      \"ie\",\"im\",\"immediate\",\"immediately\",\"importance\",\"important\",\"inc\",\"indeed\",\"index\",\"information\",\n",
    "                      \"instead\",\"invention\",\"inward\",\"itd\",\"it'll\",\"j\",\"k\",\"keep\",\"keeps\",\"kept\",\"kg\",\"km\",\"know\",\n",
    "                      \"known\",\"knows\",\"l\",\"largely\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\n",
    "                      \"let\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"'ll\",\"look\",\"looking\",\"looks\",\"ltd\",\"made\",\n",
    "                      \"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"merely\",\"mg\",\n",
    "                      \"might\",\"million\",\"miss\",\"ml\",\"moreover\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"n\",\"na\",\"name\",\n",
    "                      \"namely\",\"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\n",
    "                      \"nevertheless\",\"new\",\"next\",\"nine\",\"ninety\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"normally\",\n",
    "                      \"nos\",\"noted\",\"nothing\",\"nowhere\",\"obtain\",\"obtained\",\"obviously\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\n",
    "                      \"omitted\",\"one\",\"ones\",\"onto\",\"ord\",\"others\",\"otherwise\",\"outside\",\"overall\",\"owing\",\"p\",\"page\",\n",
    "                      \"pages\",\"part\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"poorly\",\n",
    "                      \"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\"primarily\",\n",
    "                      \"probably\",\"promptly\",\"proud\",\"provides\",\"put\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\n",
    "                      \"rd\",\"readily\",\"really\",\"recent\",\"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\n",
    "                      \"related\",\"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\"run\",\n",
    "                      \"said\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\n",
    "                      \"seen\",\"self\",\"selves\",\"sent\",\"seven\",\"several\",\"shall\",\"shed\",\"shes\",\"show\",\"showed\",\"shown\",\n",
    "                      \"showns\",\"shows\",\"significant\",\"significantly\",\"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\n",
    "                      \"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\n",
    "                      \"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\"specifying\",\"still\",\"stop\",\"strongly\",\"sub\",\n",
    "                      \"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\"tell\",\n",
    "                      \"tends\",\"th\",\"thank\",\"thanks\",\"thanx\",\"thats\",\"that've\",\"thence\",\"thereafter\",\"thereby\",\"thered\",\n",
    "                      \"therefore\",\"therein\",\"there'll\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"there've\",\n",
    "                      \"theyd\",\"theyre\",\"think\",\"thou\",\"though\",\"thoughh\",\"thousand\",\"throug\",\"throughout\",\"thru\",\"thus\",\n",
    "                      \"til\",\"tip\",\"together\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\n",
    "                      \"two\",\"u\",\"un\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"unto\",\"upon\",\"ups\",\"us\",\"use\",\"used\",\n",
    "                      \"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"'ve\",\"via\",\"viz\",\n",
    "                      \"vol\",\"vols\",\"vs\",\"w\",\"want\",\"wants\",\"wasnt\",\"way\",\"wed\",\"welcome\",\"went\",\"werent\",\"whatever\",\n",
    "                      \"what'll\",\"whats\",\"whence\",\"whenever\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\n",
    "                      \"whereupon\",\"wherever\",\"whether\",\"whim\",\"whither\",\"whod\",\"whoever\",\"whole\",\"who'll\",\"whomever\",\n",
    "                      \"whos\",\"whose\",\"widely\",\"willing\",\"wish\",\"within\",\"without\",\"wont\",\"words\",\"world\",\"wouldnt\",\"www\",\n",
    "                      \"x\",\"yes\",\"yet\",\"youd\",\"youre\",\"z\",\"zero\",\"a's\",\"ain't\",\"allow\",\"allows\",\"apart\",\"appear\",\n",
    "                      \"appreciate\",\"appropriate\",\"associated\",\"best\",\"better\",\"c'mon\",\"c's\",\"cant\",\"changes\",\"clearly\",\n",
    "                      \"concerning\",\"consequently\",\"consider\",\"considering\",\"corresponding\",\"course\",\"currently\",\n",
    "                      \"definitely\",\"described\",\"despite\",\"entirely\",\"exactly\",\"example\",\"going\",\"greetings\",\"hello\",\n",
    "                      \"help\",\"hopefully\",\"ignored\",\"inasmuch\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"it'd\",\n",
    "                      \"keep\",\"keeps\",\"novel\",\"presumably\",\"reasonably\",\"second\",\"secondly\",\"sensible\",\"serious\",\n",
    "                      \"seriously\",\"sure\",\"t's\",\"third\",\"thorough\",\"thoroughly\",\"three\",\"well\",\"wonder\"]\n",
    "\n",
    "    # stop-words\n",
    "    swcount = len(stopwords_list)\n",
    "    corpus_length = len(corpus)\n",
    "\n",
    "    for comment in corpus:\n",
    "        for sword in stopwords_list:\n",
    "            if sword in comment:\n",
    "                comment = comment.replace(sword,'')\n",
    "#     corpus = corpDenoising(corpus)\n",
    "    return corpus\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "# Function to write test predictions to .csv file\n",
    "\n",
    "def csvWriter(prediction, submission_no):\n",
    "    index = 0\n",
    "    filename = 'G_24_submission_' + str(submission_no) + '.csv'\n",
    "    csv = open(filename, \"w\") \n",
    "\n",
    "    columnTitleRow = \"Id,Category\\n\"\n",
    "    csv.write(columnTitleRow)\n",
    "    \n",
    "    for i in prediction:\n",
    "        csv.write(str(index) + ',' + str(i) + \"\\n\")\n",
    "        index+=1\n",
    "    \n",
    "    csv.close()\n",
    "    \n",
    "#########################################################################################################################\n",
    "\n",
    "# Lemma Tokenizer class for lemmatization\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "    \n",
    "#########################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation and partitioning\n",
    "\n",
    "portion = 0.85\n",
    "N_train = int(portion * Nt)\n",
    "N_valid = int((1-portion) * Nt)\n",
    "\n",
    "pos_root = 'train/pos/'\n",
    "neg_root = 'train/neg/'\n",
    "test_root = 'test/'\n",
    "\n",
    "All_pos = os.listdir(pos_root)\n",
    "All_neg = os.listdir(neg_root)\n",
    "All_test = sorted(os.listdir(test_root),key=lambda x: int(os.path.splitext(x)[0]))\n",
    "\n",
    "pos_corpus = Corpus(All_pos, pos_root)\n",
    "neg_corpus = Corpus(All_neg, neg_root)\n",
    "\n",
    "Np = len(pos_corpus)\n",
    "Nt = 2 * Np\n",
    "\n",
    "train_corpus = [None] * Nt\n",
    "Y_train = [None] * Nt\n",
    "\n",
    "for i in range(len(pos_corpus) * 2):\n",
    "    if i % 2 == 0:\n",
    "        train_corpus[i] = pos_corpus[int(i/2)]\n",
    "        Y_train[i] = 1\n",
    "    else:\n",
    "        train_corpus[i] = neg_corpus[int(math.floor(i/2))]\n",
    "        Y_train[i] = 0\n",
    "\n",
    "test_corpus = Corpus(All_test, test_root)\n",
    "\n",
    "# Removing stop-words from Training set (comment if you don't want to delete stopwords)\n",
    "\n",
    "train_corpus = delStopWords(train_corpus)\n",
    "   \n",
    "# Before splitting training set and validation set, we perform random shuffle\n",
    "\n",
    "mixed_train = list(zip(train_corpus, Y_train))\n",
    "\n",
    "random.shuffle(mixed_train)\n",
    "\n",
    "train_corpus, Y_train = zip(*mixed_train)\n",
    "\n",
    "X_training = train_corpus[:N_train]\n",
    "Y_training = Y_train[:N_train]\n",
    "X_validation = train_corpus[N_train:]\n",
    "Y_validation = Y_train[N_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1305927\n"
     ]
    }
   ],
   "source": [
    "# This block is for counting number of features \n",
    "\n",
    "cw = CountVectorizer(ngram_range=(1,2),tokenizer=word_tokenize)\n",
    "\n",
    "X_1 = cw.fit_transform(X_training)\n",
    "print(X_1.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy for BernoulliNB with binary features is: 0.8901333333333333\n",
      "Validation Accuracy for BernoulliNB with TfIDF is: 0.8901333333333333\n"
     ]
    }
   ],
   "source": [
    "# Naive-Bayes in Scikit-Learn\n",
    "\n",
    "# 1) using binary features\n",
    "\n",
    "NB_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2),tokenizer=LemmaTokenizer(),binary=True)),\n",
    "                    ('clf', BernoulliNB())])\n",
    "\n",
    "NB_clf = NB_clf.fit(X_training, Y_training)\n",
    "Y_pred_NB = NB_clf.predict(X_validation)\n",
    "Accuracy = np.sum(np.logical_not(np.logical_xor(Y_validation,Y_pred_NB)))/N_valid\n",
    "print('Validation Accuracy for BernoulliNB with binary features is: ' + str(Accuracy))\n",
    "\n",
    "\n",
    "# 2) using TfIDF \n",
    "\n",
    "NB_clf = Pipeline([ ('tfidf', TfidfVectorizer(ngram_range=(1,2),tokenizer=LemmaTokenizer(),sublinear_tf=True)),\n",
    "                    ('norm', Normalizer()),\n",
    "                    ('clf', BernoulliNB())])\n",
    "\n",
    "NB_clf = NB_clf.fit(X_training, Y_training)\n",
    "Y_pred_NB = NB_clf.predict(X_validation)\n",
    "Accuracy = np.sum(np.logical_not(np.logical_xor(Y_validation,Y_pred_NB)))/N_valid\n",
    "print('Validation Accuracy for BernoulliNB with TfIDF is: ' + str(Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34-NN pipeline created!\n",
      "34-NN Model fitted!\n",
      "Validation Accuracy for 34-NN with binary features is: 0.6576\n",
      "34-NN pipeline created!\n",
      "34-NN Model fitted!\n",
      "Validation Accuracy for 34-NN with TfIDF is: 0.8786666666666667\n"
     ]
    }
   ],
   "source": [
    "# KNN Pipeline\n",
    "\n",
    "K = 34\n",
    "\n",
    "# 1) using binary features\n",
    "\n",
    "KNN_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2),tokenizer=LemmaTokenizer(), binary=True)),\n",
    "                    ('clf', neighbors.KNeighborsClassifier(n_neighbors= K, weights= 'distance'))])\n",
    "\n",
    "print(str(K) + \"-NN pipeline created!\")\n",
    "\n",
    "KNN_clf.fit(X_training, Y_training)\n",
    "\n",
    "print((str(K) + \"-NN Model fitted!\"))\n",
    "\n",
    "Y_pred_KNN = KNN_clf.predict(X_validation)\n",
    "KNN_Accuracy = np.sum(np.logical_not(np.logical_xor(Y_validation,Y_pred_KNN)))/N_valid\n",
    "print('Validation Accuracy for ' + str(K) + '-NN with binary features is: '+ str(KNN_Accuracy))\n",
    "\n",
    "\n",
    "# 2) using TfIDF \n",
    "\n",
    "KNN_clf = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1,2), tokenizer=LemmaTokenizer(), sublinear_tf=True)),\n",
    "                    ('norm', Normalizer()),\n",
    "                    ('clf', neighbors.KNeighborsClassifier(n_neighbors= K, weights= 'distance'))])\n",
    "\n",
    "print(str(K) + \"-NN pipeline created!\")\n",
    "\n",
    "KNN_clf.fit(X_training, Y_training)\n",
    "\n",
    "print((str(K) + \"-NN Model fitted!\"))\n",
    "\n",
    "Y_pred_KNN = KNN_clf.predict(X_validation)\n",
    "KNN_Accuracy = np.sum(np.logical_not(np.logical_xor(Y_validation,Y_pred_KNN)))/N_valid\n",
    "print('Validation Accuracy for ' + str(K) + '-NN with TfIDF is: '+ str(KNN_Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__n_neighbors': 34}\n",
      "0.8501999999999998\n"
     ]
    }
   ],
   "source": [
    "# KNN pipeline with Grid search\n",
    "\n",
    "KNN_parameters = {'clf__n_neighbors': list(range(25,35))}\n",
    "\n",
    "GS_KNN_clf = GridSearchCV(KNN_clf, KNN_parameters, cv=5, iid=False, n_jobs=-1)\n",
    "GS_KNN_clf = GS_KNN_clf.fit(train_corpus, Y_train)\n",
    "\n",
    "print(GS_KNN_clf.best_params_)\n",
    "print(GS_KNN_clf.best_score_)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree pipeline created!\n",
      "Decision Tree Model fitted!\n",
      "Validation Accuracy for Decision Tree with binary features is: 0.7224\n",
      "Decision Tree pipeline created!\n",
      "Decision Tree Model fitted!\n",
      "Validation Accuracy for Decision Tree with TfIDF is: 0.7045333333333333\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree pipeline\n",
    "\n",
    "# 1) using binary features\n",
    "\n",
    "DT_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2),tokenizer=LemmaTokenizer(), binary=True)),\n",
    "                    ('clf', tree.DecisionTreeClassifier())])\n",
    "\n",
    "print(\"Decision Tree pipeline created!\")\n",
    "\n",
    "DT_clf.fit(X_training, Y_training)\n",
    "\n",
    "print(\"Decision Tree Model fitted!\")\n",
    "\n",
    "Y_pred_DT = DT_clf.predict(X_validation)\n",
    "\n",
    "DT_Accuracy = np.sum(np.logical_not(np.logical_xor(Y_validation,Y_pred_DT)))/N_valid\n",
    "print('Validation Accuracy for Decision Tree with binary features is: '+ str(DT_Accuracy))\n",
    "\n",
    "\n",
    "# 2) using TfIDF \n",
    "\n",
    "DT_clf = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1,2),tokenizer=LemmaTokenizer(),sublinear_tf=True)),\n",
    "                    ('norm', Normalizer()),\n",
    "                    ('clf', tree.DecisionTreeClassifier())])\n",
    "\n",
    "print(\"Decision Tree pipeline created!\")\n",
    "\n",
    "DT_clf.fit(X_training, Y_training)\n",
    "\n",
    "print(\"Decision Tree Model fitted!\")\n",
    "\n",
    "Y_pred_DT = DT_clf.predict(X_validation)\n",
    "\n",
    "DT_Accuracy = np.sum(np.logical_not(np.logical_xor(Y_validation,Y_pred_DT)))/N_valid\n",
    "print('Validation Accuracy for Decision Tree with TfIDF is: '+ str(DT_Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM pipeline created!\n",
      "SVM Model fitted!\n",
      "Validation Accuracy for SVM with binary features is: 0.8957333333333334\n",
      "SVM pipeline created!\n",
      "SVM Model fitted!\n",
      "Validation Accuracy for SVM with TfIDF is: 0.9154666666666667\n"
     ]
    }
   ],
   "source": [
    "# SVM pipeline\n",
    "\n",
    "# 1) using binary features\n",
    "\n",
    "SVM_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), tokenizer=LemmaTokenizer(), binary=True)),\n",
    "                    ('clf', SGDClassifier(loss='squared_hinge', alpha=1e-4, max_iter=70, tol=0.18 ))])\n",
    "\n",
    "print(\"SVM pipeline created!\")\n",
    "\n",
    "SVM_clf.fit(X_training, Y_training)\n",
    "\n",
    "print(\"SVM Model fitted!\") \n",
    "\n",
    "Y_pred_SVM = SVM_clf.predict(X_validation)\n",
    "\n",
    "SVM_Accuracy = np.sum(np.logical_not(np.logical_xor(Y_validation,Y_pred_SVM)))/N_valid\n",
    "print('Validation Accuracy for SVM with binary features is: '+ str(SVM_Accuracy))\n",
    "\n",
    "\n",
    "# 2) using TfIDF \n",
    "\n",
    "SVM_clf = Pipeline([('tfidf', TfidfVectorizer(sublinear_tf=True, ngram_range=(1,2),tokenizer=LemmaTokenizer())),\n",
    "                    ('norm', Normalizer()),\n",
    "                    ('clf', SGDClassifier(loss='squared_hinge',alpha=1e-4, max_iter=70, tol=0.18 ))])\n",
    "                    \n",
    "print(\"SVM pipeline created!\")\n",
    "\n",
    "SVM_clf.fit(X_training, Y_training)\n",
    "\n",
    "print(\"SVM Model fitted!\") \n",
    "\n",
    "Y_pred_SVM = SVM_clf.predict(X_validation)\n",
    "\n",
    "SVM_Accuracy = np.sum(np.logical_not(np.logical_xor(Y_validation,Y_pred_SVM)))/N_valid\n",
    "print('Validation Accuracy for SVM with TfIDF is: '+ str(SVM_Accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vect__tokenizer': <__main__.LemmaTokenizer object at 0x0000026CCF3A0390>}\n",
      "Best configuration score for SVM is: 0.90876\n"
     ]
    }
   ],
   "source": [
    "# SVM Grid Search\n",
    "SVM_parameters = {'vect__tokenizer': [LemmaTokenizer(),word_tokenize]}\n",
    "\n",
    "GS_SVM_clf = GridSearchCV(SVM_clf, SVM_parameters, cv=5, iid=False, n_jobs=-1) # cv = k in k-fold\n",
    "GS_SVM_clf = GS_SVM_clf.fit(train_corpus, Y_train)\n",
    "print(GS_SVM_clf.best_params_)\n",
    "# Y_pred_GS_SVM = SVM_clf.predict(X_validation)\n",
    "# GS_SVM_Accuracy = np.sum(np.logical_not(np.logical_xor(Y_validation,Y_pred_GS_SVM)))/N_valid\n",
    "print('Best configuration score for SVM is: '+ str(GS_SVM_clf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logestic Regression pipeline created!\n",
      "Model fitted!\n",
      "Validation Accuracy for Logistic Regression with binary features is: 0.8965333333333333\n",
      "Logestic Regression pipeline created!\n",
      "Model fitted!\n",
      "Validation Accuracy for Logistic Regression with TfIDF is: 0.8970666666666667\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression pipeline\n",
    "\n",
    "# 1) using binary features\n",
    "\n",
    "LogReg_clf = Pipeline([('vect', CountVectorizer(tokenizer=LemmaTokenizer(), ngram_range=(1,2), binary=True)),\n",
    "                       ('clf', LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter=500))])\n",
    "\n",
    "print(\"Logestic Regression pipeline created!\")\n",
    "LogReg_clf.fit(X_training, Y_training)\n",
    "\n",
    "print(\"Model fitted!\")\n",
    "\n",
    "Y_pred_LogReg = LogReg_clf.predict(X_validation)\n",
    "\n",
    "LogReg_Accuracy = np.sum(np.logical_not(np.logical_xor(Y_validation,Y_pred_LogReg)))/N_valid\n",
    "print('Validation Accuracy for Logistic Regression with binary features is: '+ str(LogReg_Accuracy))\n",
    "\n",
    "\n",
    "# 2) using TfIDF \n",
    "\n",
    "LogReg_clf = Pipeline([('tfidf', TfidfVectorizer(tokenizer= LemmaTokenizer(), ngram_range=(1,2), sublinear_tf=True)),\n",
    "                       ('norm', Normalizer()),\n",
    "                       ('clf', LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr', max_iter=200))])\n",
    "\n",
    "print(\"Logestic Regression pipeline created!\")\n",
    "LogReg_clf.fit(X_training, Y_training)\n",
    "\n",
    "print(\"Model fitted!\")\n",
    "\n",
    "Y_pred_LogReg = LogReg_clf.predict(X_validation)\n",
    "\n",
    "LogReg_Accuracy = np.sum(np.logical_not(np.logical_xor(Y_validation,Y_pred_LogReg)))/N_valid\n",
    "print('Validation Accuracy for Logistic Regression with TfIDF is: '+ str(LogReg_Accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on Test set and creating CSV file for Kaggle \n",
    "\n",
    "best_predictor = SVM_clf.predict(test_corpus)\n",
    "csvWriter(best_predictor,8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
